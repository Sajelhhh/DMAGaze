# DMAGaze

Gaze estimation, which predicts gaze direction, commonly faces the challenge of interference from complex gaze-irrelevant information in facial images. In this work, we propose DMAGaze, a novel gaze estimation framework that exploits information from facial images in three aspects: gaze-relevant global features (disentangled from facial image), local eye features (extracted from cropped eye patch), and head pose estimation features, to improve the overall performance of gaze estimation. Firstly, we design a new continuous mask-based disentangler to accurately disentangle gaze-relevant and -irrelevant information in facial images. Furthermore, we introduce a new cascaded attention module named Multi-Scale Global Local Attention Module (MS-GLAM), which effectively focuses on global and local information at multiple scales, enhancing the features from the disentangler. Finally, the disentangled global gaze-relevant features, combined with head pose and local eye features, are passed through the detection head for high-precision gaze estimation. Our proposed DMAGaze has been extensively validated on two mainstream public datasets, achieving state-of-the-art performance.
